# Ingestion Engine v2 - Implementation Plan

**Date:** December 26, 2025
**Status:** âœ… IMPLEMENTED
**Last Updated:** December 26, 2025 - Added Newsletter Stories output to AI Scoring (one-click populates BOTH Articles + Newsletter Stories tables)
**Prerequisite:** [Ingestion-Engine-v2-Full-Stack-Application-12-26-25.md](./Ingestion-Engine-v2-Full-Stack-Application-12-26-25.md)

---

## Overview

This document provides a step-by-step implementation plan for adding the RSS Ingestion Engine to the AI Editor 2.0 Full-Stack Application, following all existing patterns and conventions.

### What We're Building

A **one-button-click** RSS ingestion + AI scoring system that populates **TWO Airtable tables**:

**Output Tables:**
1. âœ… **Articles table** (`tblGumae8KDpsrWvh`) - ALL ingested articles
2. âœ… **Newsletter Stories table** (`tblY78ziWp5yhiGXp`) - BEST articles (interest_score >= 15)

**The Process:**
1. Fetches articles from 19 RSS feeds in parallel
2. **Resolves Google News redirect URLs** to get actual article URLs and source names
3. Generates unique `pivot_Id` for deduplication
4. Creates records in Airtable **Articles table**
5. Scores all new articles with Claude Sonnet AI
6. Creates records in **Newsletter Stories table** for high-interest articles
7. Runs as a background job via Redis Queue
8. Can be triggered from the dashboard UI

**One click. Two tables. Done.**

---

## ðŸ”¥ Critical Architecture Decision: Firecrawl Strategy

### The Problem
- We ingest 200+ articles daily from RSS feeds
- Firecrawl costs money per scrape (~$0.01-0.02 per page)
- Only articles with `interest_score >= 15` get decorated (threshold-based, not fixed count)

### The Solution: Delayed Firecrawl Extraction

**RSS data (headline + source + date) is SUFFICIENT for AI filtering/scoring!**

Headlines are specifically crafted to convey the core topic. For Pre-Filter (Step 1), the AI just needs to answer: "Is this article relevant to Slot X?" - headline alone works for this.

### When to Use Firecrawl

| Step | Uses Firecrawl? | Why |
|------|----------------|-----|
| Step 0 (Ingest) | âŒ NO | RSS-only, 200+ articles |
| AI Scoring | âŒ NO | Headline-based scoring (interest_score, sentiment, topic) |
| Decoration | âœ… YES | Need full content for articles with `interest_score >= 15` |

**Firecrawl API Key:** `fc-e532291bfbde4598948e8711ed2a17c3` (stored in `.env.local`)

### Cost Savings

- **Old approach:** Firecrawl all 200+ articles = ~$4/day
- **New approach:** Firecrawl only articles with `interest_score >= 15` (~10-20/day) = ~$0.20-0.40/day
- **Savings:** ~90-95% reduction in Firecrawl costs

---

## ðŸ”— Google News URL Resolution (Implemented)

### The Problem

Google News RSS feeds return redirect URLs like:
```
https://news.google.com/rss/articles/CBMiYkFVX3lxTk5iS...
```

These are NOT the actual article URLs - they redirect to the real source (Reuters, CNBC, etc.). If we store these Google News URLs, the `source_id` would be "Google News" instead of the actual publication.

### The Solution

The ingest job now:
1. Detects Google News URLs in fetched articles
2. Follows the redirects asynchronously (batches of 20)
3. Extracts the **actual source name** from the resolved URL
4. Updates both the `original_url` and `source_id` fields

### Domain-to-Source Mapping

```python
DOMAIN_TO_SOURCE = {
    "reuters.com": "Reuters",
    "cnbc.com": "CNBC",
    "theverge.com": "The Verge",
    "techcrunch.com": "TechCrunch",
    "yahoo.com": "Yahoo Finance",
    "wsj.com": "WSJ",
    "bloomberg.com": "Bloomberg",
    "nytimes.com": "New York Times",
    "bbc.com": "BBC",
    "forbes.com": "Forbes",
    # ... 30+ more mappings
}
```

### Results Tracking

The job results now include:
```json
{
  "google_news_resolved": 85,
  "articles_ingested": 127,
  "articles_found": 250
}
```

### Code Location

- `workers/jobs/ingest.py`:
  - `DOMAIN_TO_SOURCE` dict (lines 31-69)
  - `extract_source_from_url()` function
  - `resolve_google_news_url()` async function
  - `resolve_article_urls()` batch processor

---

## ðŸ¤– AI Scoring System (CRITICAL - From n8n Workflow)

### Overview

The AI Scoring system is a **background process** that runs every 20 minutes to score newly ingested articles. This is how articles get evaluated for newsletter inclusion - **NOT by picking a fixed number of "best" articles**.

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STEP 0: ONE-CLICK INGESTION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚   INGEST     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   AI SCORING     â”‚                          â”‚
â”‚  â”‚  (RSS Fetch) â”‚  needs_ai=1  â”‚ (Claude Sonnet)  â”‚                          â”‚
â”‚  â”‚   âœ… DONE    â”‚   triggers   â”‚     âœ… DONE      â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚         â”‚                              â”‚                                     â”‚
â”‚         â”‚                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â–¼                              â–¼                  â–¼                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  ARTICLES   â”‚              â”‚  ARTICLES   â”‚    â”‚ NEWSLETTER      â”‚        â”‚
â”‚   â”‚   TABLE     â”‚              â”‚   TABLE     â”‚    â”‚ STORIES TABLE   â”‚        â”‚
â”‚   â”‚ (all items) â”‚              â”‚ (+ scores)  â”‚    â”‚ (best items)    â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                   interest_score >= 15       â”‚
â”‚                                                                              â”‚
â”‚   OUTPUT 1: ALL articles       OUTPUT 2: BEST articles ready for            â”‚
â”‚   ingested to Articles         decoration in Newsletter Stories             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âœ… `needs_ai` Field - IMPLEMENTED

**Status:** `ingest.py` correctly sets `needs_ai = True` when creating articles.

This allows the AI Scoring job to query and score new articles: `{needs_ai} = 1`

**Current implementation in `workers/jobs/ingest.py` (lines 412-418):**

```python
record = {
    "pivot_Id": pivot_id,
    "original_url": article["link"],
    "source_id": article["source_id"],
    "date_ingested": datetime.now(timezone.utc).isoformat(),
    "needs_ai": True,  # âœ… Triggers AI Scoring job
}
```

### AI Scoring Trigger (n8n Workflow: `mgIuocpwH9kXvPjM`)

**Schedule:** Every 20 minutes
**Query:** `{needs_ai} = 1` from Articles table

### AI Scoring Output Fields

The Claude Sonnet AI scores each article and generates:

| Field | Type | Description |
|-------|------|-------------|
| `interest_score` | 0-25 | How important/interesting the article is |
| `sentiment` | -10 to 10 | Tone of the news (negative to positive) |
| `topic` | string | One of 18 fixed labels (see below) |
| `tags` | array | 5 short descriptive strings |
| `core_url` | string | Best source URL for the article |
| `newsletter_recommendations` | array | Fit scores per newsletter |
| `primary_newsletter_slug` | string | Best-fit newsletter |

### Topic Labels (18 Fixed Categories)

```
WORK, EDUCATION, INFRASTRUCTURE, HEALTHCARE, ENVIRONMENT,
FINANCE, RETAIL, ENTERTAINMENT, MANUFACTURING, SECURITY,
TRANSPORTATION, AGRICULTURE, LEGAL, REAL_ESTATE, ENERGY,
GOVERNMENT, COMMUNICATION, OTHER
```

### Newsletter Fit Scores

Each article gets a `fit_score` (0-25) for each newsletter:

| Newsletter Slug | Name | Focus |
|-----------------|------|-------|
| `pivot_ai` | Pivot AI | AI and technology news |
| `pivot_build` | Pivot Build | Business and construction |
| `pivot_invest` | Pivot Invest | Finance and investment |

### Interest Score Threshold (NOT "5 Best Articles")

**Articles are selected based on `interest_score >= 15`, NOT a fixed count!**

The Decoration workflow uses this filter:
```
=AND(
  {interest_score} >= 15,
  OR(
    {decoration_status} = "pending",
    {decoration_status} = ""
  )
)
```

Articles with `interest_score < 15` are marked `skipped_low_score` and not decorated.

### After AI Scoring Updates

The workflow sets:
- `needs_ai = false` (so it won't be scored again)
- All scoring fields (`interest_score`, `sentiment`, `topic`, etc.)

---

## ðŸ AI Scoring Python Worker (Replacing n8n Cron)

### Overview

Instead of running AI Scoring as a 20-minute n8n cron job, we implement it as a **Python worker job** that can run:
1. **Manually** - via dashboard button click
2. **Sequentially after ingestion** - as part of a two-job pipeline
3. **On-demand** - via API trigger

### Two-Job Sequential Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ONE-CLICK INGESTION + SCORING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   User clicks "Ingest & Score" button                                        â”‚
â”‚                    â”‚                                                         â”‚
â”‚                    â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚           JOB 1: INGEST              â”‚                                   â”‚
â”‚   â”‚   - Fetch 19 RSS feeds (parallel)    â”‚                                   â”‚
â”‚   â”‚   - Resolve Google News URLs         â”‚                                   â”‚
â”‚   â”‚   - Dedupe against existing pivot_Id â”‚                                   â”‚
â”‚   â”‚   - Write to Articles table          â”‚                                   â”‚
â”‚   â”‚   - Set needs_ai = true              â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                      â”‚                                                       â”‚
â”‚                      â–¼                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚         JOB 2: AI SCORING            â”‚                                   â”‚
â”‚   â”‚   - Query: needs_ai = true           â”‚                                   â”‚
â”‚   â”‚   - For each article:                â”‚                                   â”‚
â”‚   â”‚     â€¢ Call Claude Sonnet             â”‚                                   â”‚
â”‚   â”‚     â€¢ Generate scores + topic        â”‚                                   â”‚
â”‚   â”‚     â€¢ Update Articles table          â”‚                                   â”‚
â”‚   â”‚   - Set needs_ai = false             â”‚                                   â”‚
â”‚   â”‚   - IF interest_score >= 15:         â”‚                                   â”‚
â”‚   â”‚     â€¢ Write to Newsletter Stories    â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                              â”‚
â”‚   OUTPUT 1: Articles table - ALL articles with scores                        â”‚
â”‚   OUTPUT 2: Newsletter Stories table - BEST articles (interest_score >= 15)  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation: `workers/jobs/ai_scoring.py`

```python
"""
AI Scoring Job - Score articles using Claude Sonnet

Replaces the n8n 20-minute cron workflow (mgIuocpwH9kXvPjM).
Can run manually, on-demand, or chained after ingestion.

Target Table: Articles (tblGumae8KDpsrWvh) in Pivot Media Master
Query: {needs_ai} = 1
"""

import os
import json
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional

from pyairtable import Api
from anthropic import Anthropic

# Configuration
AIRTABLE_API_KEY = os.environ.get("AIRTABLE_API_KEY")
AIRTABLE_BASE_ID = os.environ.get("AIRTABLE_BASE_ID", "appwSozYTkrsQWUXB")
ARTICLES_TABLE = os.environ.get("AIRTABLE_ARTICLES_TABLE", "tblGumae8KDpsrWvh")
NEWSLETTER_STORIES_TABLE = "tblY78ziWp5yhiGXp"  # Output for high-interest articles
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")

# Interest score threshold for Newsletter Stories output
INTEREST_SCORE_THRESHOLD = 15

# Topic labels (18 fixed categories)
VALID_TOPICS = [
    "WORK", "EDUCATION", "INFRASTRUCTURE", "HEALTHCARE", "ENVIRONMENT",
    "FINANCE", "RETAIL", "ENTERTAINMENT", "MANUFACTURING", "SECURITY",
    "TRANSPORTATION", "AGRICULTURE", "LEGAL", "REAL_ESTATE", "ENERGY",
    "GOVERNMENT", "COMMUNICATION", "OTHER"
]

# Newsletter slugs for fit scoring
NEWSLETTERS = ["pivot_ai", "pivot_build", "pivot_invest"]


def build_scoring_prompt(article: Dict[str, Any]) -> str:
    """
    Build the Claude prompt for scoring an article.

    Args:
        article: Article record from Airtable

    Returns:
        Formatted prompt string
    """
    headline = article.get("headline") or article.get("title", "No headline")
    source = article.get("source_id", "Unknown")
    url = article.get("original_url", "")
    published = article.get("date_published", "")

    return f"""Analyze this news article and provide scoring for newsletter placement.

ARTICLE:
- Headline: {headline}
- Source: {source}
- URL: {url}
- Published: {published}

SCORING REQUIREMENTS:

1. **interest_score** (0-25): How important/interesting is this article?
   - 0-10: Low interest, generic news
   - 11-15: Moderate interest, worth noting
   - 16-20: High interest, should be considered for newsletter
   - 21-25: Exceptional, must-include story

2. **sentiment** (-10 to 10): Tone of the news
   - -10 to -5: Very negative (crisis, failure, scandal)
   - -4 to -1: Somewhat negative
   - 0: Neutral
   - 1 to 4: Somewhat positive
   - 5 to 10: Very positive (success, breakthrough, innovation)

3. **topic**: Classify into ONE of these categories:
   WORK, EDUCATION, INFRASTRUCTURE, HEALTHCARE, ENVIRONMENT,
   FINANCE, RETAIL, ENTERTAINMENT, MANUFACTURING, SECURITY,
   TRANSPORTATION, AGRICULTURE, LEGAL, REAL_ESTATE, ENERGY,
   GOVERNMENT, COMMUNICATION, OTHER

4. **tags**: Array of exactly 5 short descriptive tags

5. **newsletter_recommendations**: For each newsletter, provide a fit_score (0-25):
   - pivot_ai: AI and technology news
   - pivot_build: Business and construction
   - pivot_invest: Finance and investment

6. **primary_newsletter_slug**: Which newsletter is the best fit?

Return ONLY valid JSON (no markdown, no explanation):
{{
  "interest_score": <number 0-25>,
  "sentiment": <number -10 to 10>,
  "topic": "<one of the valid topics>",
  "tags": ["tag1", "tag2", "tag3", "tag4", "tag5"],
  "newsletter_recommendations": [
    {{"newsletter_slug": "pivot_ai", "fit_score": <0-25>}},
    {{"newsletter_slug": "pivot_build", "fit_score": <0-25>}},
    {{"newsletter_slug": "pivot_invest", "fit_score": <0-25>}}
  ],
  "primary_newsletter_slug": "<best fit newsletter>"
}}"""


def score_article(client: Anthropic, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Score a single article using Claude Sonnet.

    Args:
        client: Anthropic client
        article: Article record from Airtable

    Returns:
        Scoring results dict or None if failed
    """
    prompt = build_scoring_prompt(article)

    try:
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            temperature=0.3,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        # Parse JSON response
        content = response.content[0].text.strip()

        # Handle potential markdown code block wrapping
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
            content = content.strip()

        result = json.loads(content)

        # Validate required fields
        if not isinstance(result.get("interest_score"), (int, float)):
            raise ValueError("Missing or invalid interest_score")
        if result.get("topic") not in VALID_TOPICS:
            result["topic"] = "OTHER"

        return result

    except json.JSONDecodeError as e:
        print(f"[AI Scoring] JSON parse error for article {article.get('id')}: {e}")
        return None
    except Exception as e:
        print(f"[AI Scoring] Error scoring article {article.get('id')}: {e}")
        return None


def run_ai_scoring(batch_size: int = 50) -> Dict[str, Any]:
    """
    Main AI Scoring job function.

    Queries articles with needs_ai = true, scores them with Claude,
    and updates the records.

    Args:
        batch_size: Max articles to process per run (default 50)

    Returns:
        Results dict with counts and timing
    """
    print(f"[AI Scoring] Starting job at {datetime.utcnow().isoformat()}")
    started_at = datetime.now(timezone.utc)

    results = {
        "started_at": started_at.isoformat(),
        "articles_queried": 0,
        "articles_scored": 0,
        "articles_failed": 0,
        "articles_skipped": 0,
        "high_interest_count": 0,  # interest_score >= 15
        "errors": []
    }

    try:
        # Initialize clients
        if not AIRTABLE_API_KEY:
            raise ValueError("AIRTABLE_API_KEY not set")
        if not ANTHROPIC_API_KEY:
            raise ValueError("ANTHROPIC_API_KEY not set")

        airtable = Api(AIRTABLE_API_KEY)
        table = airtable.table(AIRTABLE_BASE_ID, ARTICLES_TABLE)
        claude = Anthropic(api_key=ANTHROPIC_API_KEY)

        # Query articles needing AI scoring
        print("[AI Scoring] Querying articles with needs_ai = true...")
        formula = "{needs_ai} = 1"
        articles = table.all(formula=formula, max_records=batch_size)

        results["articles_queried"] = len(articles)
        print(f"[AI Scoring] Found {len(articles)} articles to score")

        if not articles:
            print("[AI Scoring] No articles need scoring, exiting")
            results["completed_at"] = datetime.now(timezone.utc).isoformat()
            results["processed"] = 0
            return results

        # Score each article
        for record in articles:
            article_id = record["id"]
            fields = record["fields"]

            print(f"[AI Scoring] Scoring article: {fields.get('headline', fields.get('title', 'Unknown'))[:50]}...")

            # Score with Claude
            scores = score_article(claude, fields)

            if not scores:
                results["articles_failed"] += 1
                continue

            # Prepare update record
            update_fields = {
                "needs_ai": False,  # Mark as scored
                "interest_score": scores.get("interest_score"),
                "sentiment": scores.get("sentiment"),
                "topic": scores.get("topic"),
                "tags": json.dumps(scores.get("tags", [])),
                "primary_newsletter_slug": scores.get("primary_newsletter_slug"),
                "date_ai_scored": datetime.now(timezone.utc).isoformat(),
            }

            # Add fit scores for each newsletter
            for rec in scores.get("newsletter_recommendations", []):
                slug = rec.get("newsletter_slug")
                fit_score = rec.get("fit_score")
                if slug and fit_score is not None:
                    update_fields[f"fit_score_{slug}"] = fit_score

            # Track high-interest articles
            if scores.get("interest_score", 0) >= 15:
                results["high_interest_count"] += 1

            # Update Airtable record
            try:
                table.update(article_id, update_fields)
                results["articles_scored"] += 1
                print(f"[AI Scoring] âœ“ Scored: interest={scores.get('interest_score')}, topic={scores.get('topic')}")
            except Exception as e:
                error_msg = f"Failed to update article {article_id}: {e}"
                print(f"[AI Scoring] {error_msg}")
                results["errors"].append(error_msg)
                results["articles_failed"] += 1

        print(f"[AI Scoring] Complete: {results['articles_scored']} scored, {results['high_interest_count']} high-interest")

    except Exception as e:
        error_msg = f"AI Scoring job failed: {e}"
        print(f"[AI Scoring] {error_msg}")
        results["errors"].append(error_msg)
        import traceback
        traceback.print_exc()

    results["completed_at"] = datetime.now(timezone.utc).isoformat()
    results["processed"] = results["articles_scored"]
    return results


# Job configuration for RQ scheduler
JOB_CONFIG = {
    "func": run_ai_scoring,
    "trigger": "interval",
    "minutes": 20,  # Match n8n behavior, or set higher
    "id": "ai_scoring",
    "replace_existing": True
}
```

### Register AI Scoring Job

**File:** `workers/trigger.py`

Add to `get_job_function()`:

```python
elif step_name == 'ai_scoring':
    from jobs.ai_scoring import run_ai_scoring
    JOB_FUNCTIONS[step_name] = run_ai_scoring
```

Add to `QUEUE_MAPPING`:

```python
QUEUE_MAPPING = {
    'ingest': 'default',
    'ai_scoring': 'default',  # NEW
    'prefilter': 'default',
    # ...
}
```

Add to `VALID_STEPS`:

```python
VALID_STEPS = [
    'ingest',
    'ai_scoring',  # NEW
    'prefilter',
    # ...
]
```

### Chained Execution (Ingest â†’ Score)

To run both jobs sequentially with one button click:

**Option A: API Endpoint**

```python
# workers/trigger.py - add new endpoint

@app.route('/jobs/ingest-and-score', methods=['POST'])
def trigger_ingest_and_score():
    """Run ingestion, then AI scoring sequentially."""
    # Queue ingestion job
    from jobs.ingest import ingest_articles
    job1 = queue.enqueue(ingest_articles, job_timeout=600)

    # Queue AI scoring with dependency on ingestion
    from jobs.ai_scoring import run_ai_scoring
    job2 = queue.enqueue(run_ai_scoring, depends_on=job1, job_timeout=600)

    return jsonify({
        "success": True,
        "ingest_job_id": job1.id,
        "scoring_job_id": job2.id,
        "message": "Ingestion and scoring jobs queued"
    })
```

**Option B: Combined Job**

```python
# workers/jobs/ingest_and_score.py

from jobs.ingest import ingest_articles
from jobs.ai_scoring import run_ai_scoring

def run_ingest_and_score(debug: bool = False) -> Dict[str, Any]:
    """
    Combined job: Ingest RSS feeds, then score all new articles.

    This is the "one-click" solution that replaces:
    1. n8n Ingestion Engine (ddobfIOQeOykMUq6)
    2. n8n AI Scoring (mgIuocpwH9kXvPjM)
    """
    # Step 1: Ingest
    print("[Ingest+Score] Starting ingestion...")
    ingest_results = ingest_articles(debug=debug)

    # Step 2: Score (only if we ingested something)
    scoring_results = {"skipped": True, "reason": "No new articles"}
    if ingest_results.get("articles_ingested", 0) > 0:
        print("[Ingest+Score] Starting AI scoring...")
        scoring_results = run_ai_scoring()

    return {
        "ingestion": ingest_results,
        "scoring": scoring_results,
        "processed": ingest_results.get("articles_ingested", 0),
        "scored": scoring_results.get("articles_scored", 0),
    }
```

### Environment Variables Required

```bash
# AI Scoring specific
ANTHROPIC_API_KEY=sk-ant-...  # Claude API key

# Already required for ingestion
AIRTABLE_API_KEY=pat...
AIRTABLE_BASE_ID=appwSozYTkrsQWUXB
AIRTABLE_ARTICLES_TABLE=tblGumae8KDpsrWvh
```

### Dashboard Integration

Add Step 0.5 or update Step 0 to include scoring:

```typescript
// step-config.ts
{
  id: 0,
  name: "Ingest & Score",
  description: "Fetch articles from RSS feeds and score with AI",
  schedule: "Manual trigger or combined with ingestion",
  prompts: [], // Could add scoring prompt here
  dataTable: {
    name: "Articles",
    tableId: "tblGumae8KDpsrWvh",
    baseId: "appwSozYTkrsQWUXB"
  },
}
```

---

## Architecture Fit

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEWSLETTER PIPELINE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Step 0   â”‚â”€â”€â”€â–¶â”‚AI Scoringâ”‚â”€â”€â”€â–¶â”‚ Decorate â”‚â”€â”€â”€â–¶â”‚ Compile  â”‚â”€â”€...  â”‚
â”‚  â”‚ INGEST   â”‚    â”‚ (20 min) â”‚    â”‚+Firecrawlâ”‚    â”‚ + Send   â”‚       â”‚
â”‚  â”‚ âœ… DONE  â”‚    â”‚needs_ai=1â”‚    â”‚score>=15 â”‚    â”‚          â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚                â”‚               â”‚                            â”‚
â”‚       â–¼                â–¼               â–¼                            â”‚
â”‚  Articles Table    Updates       Firecrawl API                      â”‚
â”‚  (needs_ai=true)   scores        (threshold-based)                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The Ingestion Engine is **Step 0** - it feeds articles into the pipeline. The AI Scoring workflow runs every 20 minutes to score articles with `needs_ai = true`.

**Key Tables:**
- **Step 0 (Ingest)** â†’ Articles table (`tblGumae8KDpsrWvh`) in Pivot Media Master, sets `needs_ai = true`
- **AI Scoring** â†’ Updates Articles table with scores, sets `needs_ai = false`
- **Decoration** â†’ Firecrawl extraction for articles with `interest_score >= 15`

---

## Implementation Checklist

### Phase 1: Worker Backend (Python) âœ… COMPLETE

- [x] 1.1 Create `workers/config/rss_feeds.py`
- [x] 1.2 Create `workers/utils/pivot_id.py`
- [x] 1.3 Create `workers/jobs/ingest.py` (with Google News URL resolution!)
- [x] 1.4 Update `workers/jobs/__init__.py`
- [x] 1.5 Update `workers/trigger.py` to register job
- [x] 1.6 Update `workers/utils/airtable.py` with ingestion methods

### Phase 2: API Routes (Next.js) âœ… COMPLETE

- [x] 2.1 API routes already exist (jobs trigger via `/api/jobs`)
- [x] 2.2 Job status routes work via step pages

### Phase 3: Frontend UI (React) âœ… COMPLETE

- [x] 3.1 Step page works for triggering ingest job
- [x] 3.2 Dashboard shows ingest as Step 0

### Phase 4: Configuration âœ… COMPLETE

- [x] 4.1 render.yaml configured
- [x] 4.2 step-config.ts includes Step 0

### Phase 5: Testing âœ… COMPLETE

- [x] 5.1 Deployed and tested on Render
- [x] 5.2 Single feed ingestion works (debug mode)
- [x] 5.3 Full ingestion run works (19 feeds, 200+ articles)
- [x] 5.4 Airtable records created correctly in Articles table
- [x] 5.5 Deduplication verified (pivot_Id matching)

### Phase 6: Deployment âœ… COMPLETE

- [x] 6.1 Pushed to GitHub
- [x] 6.2 Render auto-deployed
- [x] 6.3 Production tested and working

---

## Phase 1: Worker Backend

### 1.1 Create RSS Feeds Configuration

**File:** `workers/config/rss_feeds.py`

```python
"""
RSS Feed Configuration for Ingestion Engine
All feeds are fetched in parallel during ingestion.
"""

RSS_FEEDS = [
    # Reuters
    {"name": "Reuters AI", "url": "https://rss.app/feeds/MXiuPVkXDT8HqezK.xml", "source_id": "Reuters"},
    {"name": "Reuters Business", "url": "https://rss.app/feeds/C3YLADfGxE5e57eT.xml", "source_id": "Reuters"},

    # AI Newsletters
    {"name": "The Neuron", "url": "https://rss.app/feeds/1iWmVmkwOR9FvPtW.xml", "source_id": "The Neuron"},
    {"name": "AI Valley", "url": "https://rss.app/feeds/el3M8L2iqw3VrU3A.xml", "source_id": "AI Valley"},
    {"name": "There's an AI For That", "url": "https://rss.app/feeds/9SVrxNsg7y419Fke.xml", "source_id": "There's an AI For That"},
    {"name": "The Deep View", "url": "https://rss.app/feeds/NY8oNua0ZxWUYR3Z.xml", "source_id": "The Deep View"},
    {"name": "The AI Report", "url": "https://rss.app/feeds/kRbnlccEQPpl1f6M.xml", "source_id": "The AI Report"},
    {"name": "SuperHuman", "url": "https://rss.app/feeds/QymucjzuFkzvxvkg.xml", "source_id": "SuperHuman"},

    # Tech News
    {"name": "The Verge", "url": "https://rss.app/feeds/08AqYC4pZsuLfMKv.xml", "source_id": "The Verge"},
    {"name": "TechCrunch", "url": "https://rss.app/feeds/YaCBpvEvBDczG9zT.xml", "source_id": "TechCrunch"},
    {"name": "Tech Republic", "url": "https://rss.app/feeds/mC6cK6lSVgJjRTgO.xml", "source_id": "Tech Republic"},
    {"name": "The Atlantic Technology", "url": "https://rss.app/feeds/L83urFREcjBOcQ5z.xml", "source_id": "The Atlantic"},

    # Finance
    {"name": "CNBC Finance", "url": "https://rss.app/feeds/yD81szEq5uTWg5I5.xml", "source_id": "CNBC"},
    {"name": "Yahoo Finance", "url": "https://news.yahoo.com/rss/finance", "source_id": "Yahoo Finance"},

    # Semafor
    {"name": "Semafor Business", "url": "https://rss.app/feeds/ZbdBsJTYo3gDOWmI.xml", "source_id": "Semafor"},
    {"name": "Semafor Technology", "url": "https://rss.app/feeds/6GwMn0gNjbWxUjPN.xml", "source_id": "Semafor"},
    {"name": "Semafor CEO", "url": "https://rss.app/feeds/jSkbNDntFNSdShkz.xml", "source_id": "Semafor"},

    # Google News Aggregators
    {"name": "Google News AI", "url": "https://news.google.com/rss/search?q=AI+OR+%22artificial+intelligence%22+when:12h&hl=en-US&gl=US&ceid=US:en", "source_id": "Google News"},
    {"name": "Google News Finance", "url": "https://news.google.com/rss/search?q=markets+OR+%22S%26P+500%22+OR+stocks+OR+earnings+when:12h&hl=en-US&gl=US&ceid=US:en", "source_id": "Google News"},
]

# For debugging - can limit to specific feeds
DEBUG_FEEDS = [
    {"name": "Yahoo Finance", "url": "https://news.yahoo.com/rss/finance", "source_id": "Yahoo Finance"},
]
```

---

### 1.2 Create PivotID Utilities

**File:** `workers/utils/pivot_id.py`

```python
"""
PivotID Generation Utilities

Generates unique hash-based identifiers for article deduplication.
Uses DJB2 algorithm to match JavaScript implementation from n8n workflow.
"""

from urllib.parse import urlparse, urlunparse, parse_qs, urlencode
from typing import Optional


def hash_string(s: str) -> str:
    """
    DJB2 hash algorithm - matches JavaScript implementation.

    Args:
        s: String to hash

    Returns:
        Base36 encoded hash string
    """
    if not isinstance(s, str):
        s = str(s) if s else ''

    hash_value = 5381
    for char in s:
        hash_value = ((hash_value << 5) + hash_value) + ord(char)
        hash_value = hash_value & 0xFFFFFFFF  # Keep as 32-bit unsigned

    return _base36_encode(hash_value)


def _base36_encode(num: int) -> str:
    """Convert number to base36 string."""
    chars = '0123456789abcdefghijklmnopqrstuvwxyz'
    if num == 0:
        return '0'
    result = ''
    while num:
        result = chars[num % 36] + result
        num //= 36
    return result


def normalize_url(url: str) -> Optional[str]:
    """
    Normalize URL for consistent hashing.

    Removes tracking parameters (UTM, etc.) and normalizes format.

    Args:
        url: URL to normalize

    Returns:
        Normalized URL string or None if invalid
    """
    if not url:
        return None

    try:
        parsed = urlparse(url.lower())

        # Remove tracking parameters
        tracking_params = {
            'utm_source', 'utm_medium', 'utm_campaign',
            'utm_term', 'utm_content', 'ref', 'source'
        }
        query_params = parse_qs(parsed.query)
        filtered_params = {
            k: v for k, v in query_params.items()
            if k not in tracking_params
        }

        # Rebuild URL without tracking params
        cleaned = parsed._replace(
            query=urlencode(filtered_params, doseq=True),
            path=parsed.path.rstrip('/'),
            fragment=''  # Remove anchors
        )
        return urlunparse(cleaned)
    except Exception:
        return url.lower().rstrip('/')


def generate_pivot_id(url: str = None, title: str = None) -> Optional[str]:
    """
    Generate pivotId from URL or title.

    Uses normalized URL if available, falls back to title.
    Returns None if neither is provided.

    Args:
        url: Article URL (preferred)
        title: Article title (fallback)

    Returns:
        pivotId string (format: "p_<hash>") or None
    """
    normalized_url = normalize_url(url)
    pivot_base = normalized_url or title

    if not pivot_base:
        return None

    return f"p_{hash_string(pivot_base)}"


def generate_story_id(pivot_id: str) -> Optional[str]:
    """
    Generate storyId from pivotId.

    Args:
        pivot_id: The pivotId (format: "p_<hash>")

    Returns:
        storyId string (format: "s_<hash>") or None
    """
    if not pivot_id:
        return None

    return pivot_id.replace("p_", "s_")
```

---

### 1.3 Create Ingest Job

**File:** `workers/jobs/ingest.py`

> **Note:** The actual implementation is more comprehensive than this example. See the full source file for Google News URL resolution and all features.

```python
"""
Step 0: RSS Ingestion Job

Fetches articles from RSS feeds and creates records in Airtable.
This is Step 0 of the newsletter pipeline - raw ingestion to Articles table.

ARCHITECTURE:
  Step 0 (Ingest) â†’ Articles table (raw RSS data)
  Step 1 (Pre-Filter) â†’ Reads from Articles, AI scores â†’ Pre-Filter Log
  Step 2 (Slot Selection) â†’ Selects best articles â†’ Newsletter Issue Stories

Target Table: Articles (tblGumae8KDpsrWvh) in Pivot Media Master
"""

import asyncio
import aiohttp
import feedparser
import os
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from pyairtable import Api
from urllib.parse import urlparse

from utils.pivot_id import generate_pivot_id
from config.rss_feeds import get_feeds


# Airtable configuration - ARTICLES TABLE (not Newsletter Issue Stories!)
AIRTABLE_API_KEY = os.environ.get("AIRTABLE_API_KEY")
AIRTABLE_BASE_ID = os.environ.get("AIRTABLE_BASE_ID", "appwSozYTkrsQWUXB")  # Pivot Media Master
ARTICLES_TABLE = os.environ.get(
    "AIRTABLE_ARTICLES_TABLE",
    "tblGumae8KDpsrWvh"  # Articles table - raw ingestion target
)


async def fetch_feed(
    session: aiohttp.ClientSession,
    feed: Dict[str, str]
) -> List[Dict[str, Any]]:
    """
    Fetch and parse a single RSS feed.

    Args:
        session: aiohttp client session
        feed: Feed configuration dict with name, url, source_id

    Returns:
        List of parsed article dicts
    """
    try:
        async with session.get(feed["url"], timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status != 200:
                print(f"[Ingest] Error fetching {feed['name']}: HTTP {response.status}")
                return []

            content = await response.text()
            parsed = feedparser.parse(content)

            articles = []
            for entry in parsed.entries:
                # Extract image URL from various RSS formats
                image_url = None
                if hasattr(entry, 'media_content') and entry.media_content:
                    image_url = entry.media_content[0].get('url')
                elif hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            image_url = enc.get('href')
                            break

                # Extract categories/tags
                categories = []
                if hasattr(entry, 'tags'):
                    categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]

                articles.append({
                    "title": entry.get("title", "").strip(),
                    "link": entry.get("link", "").strip(),
                    "pubDate": entry.get("published", ""),
                    "categories": ", ".join(categories) if categories else None,
                    "image_url": image_url,
                    "source_id": feed["source_id"],
                    "feed_name": feed["name"]
                })

            print(f"[Ingest] Fetched {len(articles)} articles from {feed['name']}")
            return articles

    except asyncio.TimeoutError:
        print(f"[Ingest] Timeout fetching {feed['name']}")
        return []
    except Exception as e:
        print(f"[Ingest] Error fetching {feed['name']}: {e}")
        return []


async def fetch_all_feeds(feeds: List[Dict[str, str]] = None) -> List[Dict[str, Any]]:
    """
    Fetch all RSS feeds in parallel.

    Args:
        feeds: List of feed configs (defaults to RSS_FEEDS)

    Returns:
        Flattened list of all articles from all feeds
    """
    if feeds is None:
        feeds = RSS_FEEDS

    print(f"[Ingest] Fetching {len(feeds)} RSS feeds in parallel...")

    async with aiohttp.ClientSession() as session:
        tasks = [fetch_feed(session, feed) for feed in feeds]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Flatten results, filtering out exceptions
        all_articles = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"[Ingest] Feed {feeds[i]['name']} failed: {result}")
            elif isinstance(result, list):
                all_articles.extend(result)

        return all_articles


def ingest_articles(debug: bool = False) -> Dict[str, Any]:
    """
    Main ingestion job function.

    Fetches all RSS feeds, resolves Google News URLs,
    deduplicates against existing records, and creates
    new records in Airtable Articles table.

    Args:
        debug: If True, only fetch from DEBUG_FEEDS (single feed for testing)

    Returns:
        Results dict with counts and timing
    """
    print(f"[Ingest] Starting ingestion job at {datetime.utcnow().isoformat()}")
    started_at = datetime.now(timezone.utc)

    results = {
        "started_at": started_at.isoformat(),
        "feeds_count": 0,
        "articles_found": 0,
        "articles_ingested": 0,
        "articles_skipped_duplicate": 0,
        "articles_skipped_invalid": 0,
        "google_news_resolved": 0,  # Track URL resolutions
        "errors": []
    }

    try:
        # Initialize Airtable - ARTICLES TABLE (raw RSS data)
        api = Api(AIRTABLE_API_KEY)
        table = api.table(AIRTABLE_BASE_ID, ARTICLES_TABLE)

        # Select feeds based on debug mode
        feeds = get_feeds(debug=debug)
        results["feeds_count"] = len(feeds)

        # Fetch all articles (includes Google News URL resolution)
        articles, google_news_resolved = asyncio.run(fetch_all_feeds(feeds))
        results["articles_found"] = len(articles)
        results["google_news_resolved"] = google_news_resolved

        # ... deduplication and record creation ...

        # Prepare Airtable record - ARTICLES TABLE SCHEMA
        record = {
            "pivot_Id": pivot_id,           # Primary deduplication key
            "original_url": article["link"], # Source URL (resolved if Google News)
            "source_id": article["source_id"], # Actual publication name
            "date_ingested": datetime.now(timezone.utc).isoformat(),
        }

        # Note: We don't have markdown since we're RSS-only (no Firecrawl)
        # Firecrawl happens in Step 3 (Decoration) for selected articles only

    except Exception as e:
        # ... error handling ...

    results["completed_at"] = datetime.now(timezone.utc).isoformat()

    # CRITICAL: Add 'processed' key for UI compatibility
    # UI looks for 'processed' or 'total_written', not 'articles_ingested'
    results["processed"] = results["articles_ingested"]

    return results
```

---

### 1.4 Update Jobs __init__.py

**File:** `workers/jobs/__init__.py`

Add the import:

```python
from .ingest import ingest_articles

__all__ = [
    # ... existing exports ...
    'ingest_articles',
]
```

---

### 1.5 Register Job in Trigger Service

**File:** `workers/trigger.py`

Add to the `get_job_function()` function (around line 59-99):

```python
elif step_name == 'ingest':
    from jobs.ingest import ingest_articles
    JOB_FUNCTIONS[step_name] = ingest_articles
```

Add to the `QUEUE_MAPPING` dict (around line 102-111):

```python
QUEUE_MAPPING = {
    'ingest': 'default',          # Step 0 - RSS Ingestion
    'prefilter': 'default',       # Step 1
    'slot_selection': 'high',     # Step 2
    # ... rest unchanged ...
}
```

Add to `VALID_STEPS` list:

```python
VALID_STEPS = [
    'ingest',        # NEW
    'prefilter',
    'slot_selection',
    # ...
]
```

---

### 1.6 Add Airtable Helper Methods (Optional)

**File:** `workers/utils/airtable.py`

Add these methods to the `AirtableClient` class:

```python
def get_newsletter_issue_stories_table(self):
    """Get Newsletter Issue Stories table reference."""
    return self.api.table(
        self.master_base_id,
        os.environ.get('AIRTABLE_NEWSLETTER_ISSUE_STORIES_TABLE', 'tblaHcFFG6Iw3w7lL')
    )

def get_existing_pivot_ids(self) -> set:
    """Get all existing pivotIds for deduplication."""
    table = self.get_newsletter_issue_stories_table()
    records = table.all(fields=["pivotId"])
    return {r["fields"].get("pivotId") for r in records if r["fields"].get("pivotId")}

def create_ingested_article(self, record: dict) -> dict:
    """Create a new record in Newsletter Issue Stories table."""
    table = self.get_newsletter_issue_stories_table()
    return table.create(record)
```

---

## Phase 2: API Routes

### 2.1 Create Ingest API Route

**File:** `app/src/app/api/ingest/route.ts`

```typescript
import { NextRequest, NextResponse } from "next/server";

const TRIGGER_SERVICE_URL = process.env.TRIGGER_SERVICE_URL || "http://localhost:5001";
const TRIGGER_SECRET = process.env.TRIGGER_SECRET;

/**
 * POST /api/ingest
 *
 * Triggers the RSS ingestion job.
 *
 * Request body (optional):
 * {
 *   debug?: boolean  // If true, only fetch from debug feeds
 * }
 */
export async function POST(request: NextRequest) {
  try {
    const body = await request.json().catch(() => ({}));
    const { debug = false } = body;

    const triggerUrl = `${TRIGGER_SERVICE_URL}/jobs/ingest`;

    const response = await fetch(triggerUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(TRIGGER_SECRET && { Authorization: `Bearer ${TRIGGER_SECRET}` }),
      },
      body: JSON.stringify({ debug }),
    });

    const data = await response.json();

    if (!response.ok) {
      return NextResponse.json(
        { success: false, error: data.error || "Failed to trigger ingestion" },
        { status: response.status }
      );
    }

    return NextResponse.json({
      success: true,
      job_id: data.job_id,
      queue: data.queue,
      message: "Ingestion job queued successfully",
    });
  } catch (error) {
    console.error("[/api/ingest] Error:", error);
    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      },
      { status: 500 }
    );
  }
}

/**
 * GET /api/ingest
 *
 * Get status of ingestion jobs in queue.
 */
export async function GET() {
  try {
    const triggerUrl = `${TRIGGER_SERVICE_URL}/jobs/queue`;

    const response = await fetch(triggerUrl, {
      method: "GET",
      headers: {
        ...(TRIGGER_SECRET && { Authorization: `Bearer ${TRIGGER_SECRET}` }),
      },
    });

    const data = await response.json();
    return NextResponse.json(data, { status: response.status });
  } catch (error) {
    console.error("[/api/ingest GET] Error:", error);
    return NextResponse.json(
      { error: "Failed to get queue status" },
      { status: 500 }
    );
  }
}
```

---

### 2.2 Create Ingest Job Status Route

**File:** `app/src/app/api/ingest/[jobId]/route.ts`

```typescript
import { NextRequest, NextResponse } from "next/server";

const TRIGGER_SERVICE_URL = process.env.TRIGGER_SERVICE_URL || "http://localhost:5001";
const TRIGGER_SECRET = process.env.TRIGGER_SECRET;

/**
 * GET /api/ingest/[jobId]
 *
 * Get status of a specific ingestion job.
 */
export async function GET(
  request: NextRequest,
  { params }: { params: { jobId: string } }
) {
  try {
    const { jobId } = params;

    const triggerUrl = `${TRIGGER_SERVICE_URL}/jobs/status/${jobId}`;

    const response = await fetch(triggerUrl, {
      method: "GET",
      headers: {
        ...(TRIGGER_SECRET && { Authorization: `Bearer ${TRIGGER_SECRET}` }),
      },
    });

    const data = await response.json();
    return NextResponse.json(data, { status: response.status });
  } catch (error) {
    console.error("[/api/ingest/[jobId]] Error:", error);
    return NextResponse.json(
      { error: "Failed to get job status" },
      { status: 500 }
    );
  }
}
```

---

## Phase 3: Frontend UI

### 3.1 Create Ingest Button Component

**File:** `app/src/components/IngestButton.tsx`

```typescript
"use client";

import { useState } from "react";

interface IngestResult {
  success: boolean;
  job_id?: string;
  error?: string;
}

interface JobStatus {
  status: "queued" | "started" | "finished" | "failed";
  result?: {
    articles_found: number;
    articles_ingested: number;
    articles_skipped_duplicate: number;
    articles_skipped_invalid: number;
    errors: string[];
  };
}

export default function IngestButton() {
  const [loading, setLoading] = useState(false);
  const [jobId, setJobId] = useState<string | null>(null);
  const [status, setStatus] = useState<JobStatus | null>(null);
  const [error, setError] = useState<string | null>(null);

  const triggerIngestion = async (debug: boolean = false) => {
    setLoading(true);
    setError(null);
    setStatus(null);

    try {
      const response = await fetch("/api/ingest", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ debug }),
      });

      const data: IngestResult = await response.json();

      if (!data.success) {
        setError(data.error || "Failed to trigger ingestion");
        return;
      }

      setJobId(data.job_id || null);

      // Start polling for status
      if (data.job_id) {
        pollJobStatus(data.job_id);
      }
    } catch (err) {
      setError(err instanceof Error ? err.message : "Unknown error");
    } finally {
      setLoading(false);
    }
  };

  const pollJobStatus = async (id: string) => {
    const maxAttempts = 60; // 5 minutes at 5-second intervals
    let attempts = 0;

    const poll = async () => {
      if (attempts >= maxAttempts) {
        setError("Job timed out");
        return;
      }

      try {
        const response = await fetch(`/api/ingest/${id}`);
        const data: JobStatus = await response.json();

        setStatus(data);

        if (data.status === "queued" || data.status === "started") {
          attempts++;
          setTimeout(poll, 5000); // Poll every 5 seconds
        }
      } catch (err) {
        setError("Failed to get job status");
      }
    };

    poll();
  };

  return (
    <div className="p-6 bg-white rounded-lg shadow-md">
      <h2 className="text-xl font-semibold mb-4">RSS Ingestion</h2>

      <div className="flex gap-4 mb-4">
        <button
          onClick={() => triggerIngestion(false)}
          disabled={loading}
          className="px-4 py-2 bg-blue-600 text-white rounded hover:bg-blue-700 disabled:bg-gray-400"
        >
          {loading ? "Starting..." : "Ingest All Feeds"}
        </button>

        <button
          onClick={() => triggerIngestion(true)}
          disabled={loading}
          className="px-4 py-2 bg-gray-600 text-white rounded hover:bg-gray-700 disabled:bg-gray-400"
        >
          Debug (1 Feed)
        </button>
      </div>

      {error && (
        <div className="p-3 bg-red-100 text-red-700 rounded mb-4">
          {error}
        </div>
      )}

      {jobId && (
        <div className="text-sm text-gray-500 mb-2">
          Job ID: <code className="bg-gray-100 px-1 rounded">{jobId}</code>
        </div>
      )}

      {status && (
        <div className="p-4 bg-gray-50 rounded">
          <div className="flex items-center gap-2 mb-2">
            <span className="font-medium">Status:</span>
            <span
              className={`px-2 py-1 rounded text-sm ${
                status.status === "finished"
                  ? "bg-green-100 text-green-700"
                  : status.status === "failed"
                  ? "bg-red-100 text-red-700"
                  : "bg-yellow-100 text-yellow-700"
              }`}
            >
              {status.status}
            </span>
          </div>

          {status.result && (
            <div className="grid grid-cols-2 gap-2 text-sm">
              <div>Articles Found:</div>
              <div className="font-mono">{status.result.articles_found}</div>

              <div>Articles Ingested:</div>
              <div className="font-mono text-green-600">
                {status.result.articles_ingested}
              </div>

              <div>Duplicates Skipped:</div>
              <div className="font-mono text-gray-500">
                {status.result.articles_skipped_duplicate}
              </div>

              <div>Invalid Skipped:</div>
              <div className="font-mono text-gray-500">
                {status.result.articles_skipped_invalid}
              </div>
            </div>
          )}

          {status.result?.errors && status.result.errors.length > 0 && (
            <div className="mt-2 text-sm text-red-600">
              Errors: {status.result.errors.length}
            </div>
          )}
        </div>
      )}
    </div>
  );
}
```

---

## Phase 4: Configuration Updates

### 4.1 Update step-config.ts

**File:** `app/src/lib/step-config.ts`

Step 0 configuration (already implemented):

```typescript
{
  id: 0,
  name: "Ingest",
  description: "Fetch articles from RSS feeds and create records in Airtable",
  schedule: "Manual trigger only",
  prompts: [], // No prompts for ingestion
  dataTable: {
    name: "Articles",
    tableId: "tblGumae8KDpsrWvh",  // Articles table in Pivot Media Master
    baseId: "appwSozYTkrsQWUXB"
  },
}
```

---

## Phase 5: Local Testing with Docker

### 5.1 Start Local Environment

```bash
cd /Users/patsimmons/client-coding/pivot-5-website_11.19.25/ai-editor-2.0-full-stack-application

# Start all services
docker-compose up

# Or rebuild if you made changes
docker-compose up --build
```

### 5.2 Verify Services Running

| Service | URL | Purpose |
|---------|-----|---------|
| Web Dashboard | http://localhost:3001 | Next.js frontend |
| Trigger Service | http://localhost:5001 | Flask job trigger |
| Redis Commander | http://localhost:8081 | Queue monitoring |
| PostgreSQL | localhost:5432 | Database |

### 5.3 Test Ingestion Manually

```bash
# Test with debug mode (single feed)
curl -X POST http://localhost:5001/jobs/ingest \
  -H "Content-Type: application/json" \
  -d '{"debug": true}'

# Response: {"success": true, "job_id": "abc123", "queue": "default"}

# Check job status
curl http://localhost:5001/jobs/status/abc123

# Or via Next.js API
curl -X POST http://localhost:3001/api/ingest \
  -H "Content-Type: application/json" \
  -d '{"debug": true}'
```

### 5.4 Monitor Queue in Redis Commander

1. Open http://localhost:8081
2. Navigate to `rq:queue:default`
3. Watch jobs being processed

### 5.5 Verify Airtable Records

1. Open Airtable: https://airtable.com/appwSozYTkrsQWUXB/tblGumae8KDpsrWvh (Articles table)
2. Check for new records with `pivot_Id` starting with `p_`
3. Verify `original_url`, `source_id`, `date_ingested` are populated
4. For Google News articles, verify `source_id` shows actual publication (Reuters, CNBC, etc.) not "Google News"

### 5.6 Test Deduplication

```bash
# Run ingestion twice
curl -X POST http://localhost:5001/jobs/ingest -H "Content-Type: application/json" -d '{"debug": true}'
# Wait for completion
curl -X POST http://localhost:5001/jobs/ingest -H "Content-Type: application/json" -d '{"debug": true}'

# Second run should show:
# "articles_ingested": 0
# "articles_skipped_duplicate": X
```

---

## Phase 6: Render Deployment

### 6.1 Environment Variables

Ensure these are set in Render dashboard for all relevant services:

```bash
# Already configured
AIRTABLE_API_KEY=patQVZtZjQS8GU78r.xxx
AIRTABLE_BASE_ID=appwSozYTkrsQWUXB

# Articles table for Step 0 ingestion (Pivot Media Master)
AIRTABLE_ARTICLES_TABLE=tblGumae8KDpsrWvh

# Firecrawl for Step 3 decoration (not used in ingestion)
FIRECRAWL_API_KEY=fc-e532291bfbde4598948e8711ed2a17c3

# Worker/Trigger service specific
REDIS_URL=<from Render Redis>
```

### 6.2 Deploy Process

```bash
# From app/ directory (the git repo)
cd /Users/patsimmons/client-coding/pivot-5-website_11.19.25/ai-editor-2.0-full-stack-application/app

git add .
git commit -m "Add RSS Ingestion Engine (Step 0)"
git push origin main
```

Render will auto-deploy:
- Dashboard service (Next.js)
- Worker service (Python)
- Trigger service (Flask)

### 6.3 Verify Production Deployment

1. Check Render dashboard for successful deploys
2. Test via production API:
   ```bash
   curl -X POST https://your-dashboard.onrender.com/api/ingest \
     -H "Content-Type: application/json" \
     -d '{"debug": true}'
   ```
3. Check Airtable for new records

---

## Troubleshooting

### Common Issues

1. **"Connection refused" to trigger service**
   - Check TRIGGER_SERVICE_URL is correct
   - Verify trigger service is running: `docker-compose ps`

2. **"No articles ingested"**
   - Check RSS feed URLs are accessible
   - Verify Airtable API key is valid
   - Check worker logs: `docker-compose logs worker`

3. **"All articles skipped as duplicates"**
   - This is expected on second run
   - To test fresh, manually delete records from Airtable

4. **Job stuck in "queued" status**
   - Check worker is running: `docker-compose logs worker`
   - Check Redis connection: http://localhost:8081

### Logs

```bash
# View all logs
docker-compose logs -f

# View specific service
docker-compose logs -f worker
docker-compose logs -f web
```

---

## Future Enhancements

1. **Scheduled Ingestion** - Add cron schedule to run every 6 hours
2. **Feed Health Monitoring** - Track which feeds fail/succeed over time
3. **Feed Management UI** - Add/remove feeds from dashboard
4. **Missing Tech Sites** - Add Bloomberg, WSJ, NYTimes feeds when available
5. **URL Resolution Analytics** - Track which Google News URLs fail to resolve

**Note:** Content extraction via Firecrawl is intentionally NOT done at ingestion. It happens at Decoration for articles that pass the `interest_score >= 15` threshold. See "ðŸ”¥ Critical Architecture Decision: Firecrawl Strategy" and "ðŸ¤– AI Scoring System" sections above.

---

## Key Bug Fixes Applied

### 1. Article Count Display Bug
**Problem:** UI showed "processed 0 articles" even though ingestion worked.
**Cause:** UI looks for `processed` or `total_written` keys, but job returned `articles_ingested`.
**Fix:** Added `results["processed"] = results["articles_ingested"]` before returning.

### 2. Google News Source ID
**Problem:** Articles from Google News feeds showed "Google News" as source instead of actual publication.
**Cause:** RSS-only ingestion used feed config's `source_id` without resolving redirect URLs.
**Fix:** Added async URL resolution that follows redirects and extracts real source names.

### 3. Wrong Target Table
**Problem:** Initial implementation wrote to Newsletter Issue Stories table.
**Cause:** Copy-paste from different step's code.
**Fix:** Changed to Articles table (`tblGumae8KDpsrWvh`) in Pivot Media Master base.
